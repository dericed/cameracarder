#!/usr/bin/env bash

# this script finds camera cards either from an input
# supported inputs are a directory to descend in search of camera cards or a treesize report in Excel or a csv in the format of the treesize file inventory sheet

# dependencies: in2csv (to convert excel to csv), xsv (for csv parsing)
DEPENDENCIES=(in2csv xsv)

# get list of needed columns to add to documentation here
# look up csv header row and the get index numbers of column

# add appendix to report with helpful commands like find /folder -type d -empty

_check_dependencies(){
    DEPS_OK=YES
    while [ "${*}" != "" ] ; do
        DEPENDENCY="${1}"
        if [ ! $(which "${DEPENDENCY}") ] ; then
            echo "This script requires ${DEPENDENCY} to run but it is not installed"
            echo "If you are running ubuntu or debian you might be able to install ${DEPENDENCY} with the following command"
            echo "sudo apt-get install ${DEPENDENCY}"
            echo "If you are running mac you might be able to install ${DEPENDENCY} with the following command"
            echo "brew install ${DEPENDENCY}"
            DEPS_OK=NO
        fi
        shift
    done
    if [[ "${DEPS_OK}" = "NO" ]]; then
        echo "Unmet dependencies"
        echo "Exiting!"
        exit 1
    else
        return 0
    fi
}
_check_dependencies "${DEPENDENCIES[@]}"

_report_duplicates(){
    # examine files

    # option to recommend first or last for duplication
    # summary potential for deleting duplicates, sum the totals
    # add in duplicates of size and mod date, some times extra filename has "(1)"

    # have a mover to send files to new place.
    # duplicate column that is the preserved orginal, column of the deleted.
    # have a column to opt into the process. g

    INPUT_CSV="${1}"
    OUTPUT_CSV="${2}"
    echo "Working on the duplicate report now..."
    # Step 1: Parse CSV and group files by folder
    # Key for duplicates: filename + size

    declare -A FILES_IN_FOLDER  # folder -> list of "name|size"
    declare -A FILE_DETAILS     # "folder|name|size" -> "last_modified|last_accessed"
    declare -A FOLDER_FILESIZE  # folder -> total filesize

    # Read CSV skipping header
    while IFS=, read -r NAME FOLDER_PATH SIZE LAST_MODIFIED LAST_ACCESSED TYPE REST
    do
      # Remove possible quotes
      NAME="${NAME//\"/}"
      FOLDER_PATH="${FOLDER_PATH//\"/}"
      SIZE="${SIZE//\"/}"
      LAST_MODIFIED="${LAST_MODIFIED//\"/}"
      LAST_ACCESSED="${LAST_ACCESSED//\"/}"

      # Skip header line or blank lines
      [[ "$NAME" == "Name" || -z "$NAME" ]] && continue

      # Append file info to folder's list
      FILES_IN_FOLDER["$FOLDER_PATH"]+="${NAME}|${SIZE},"

      # Store details
      FILE_DETAILS["$FOLDER_PATH|$NAME|$SIZE"]="${LAST_MODIFIED}|${LAST_ACCESSED}"

      # Sum filesize
      CURRENT_SIZE=${FOLDER_FILESIZE[$FOLDER_PATH]:-0}
      FOLDER_FILESIZE[$FOLDER_PATH]=$((CURRENT_SIZE + SIZE))
    done < <(tail -n +2 "${INPUT_CSV}")

    # Convert FILES_IN_FOLDER from comma-separated string to arrays for easier matching
    # We'll store arrays indexed by folder

    declare -A FILE_ARRAYS

    for FOLDER in "${!FILES_IN_FOLDER[@]}"; do
      # Remove trailing comma and replace comma by space for array
      FILE_LIST="${FILES_IN_FOLDER[$FOLDER]}"
      FILE_LIST="${FILE_LIST%,}"
      FILE_ARRAYS["$FOLDER"]="$FILE_LIST"
    done

    # Step 2: Compare folders to find duplicates

    # We'll compare each folder with every other folder
    # Calculate percentage match = (number of duplicate files) / (number of files in folder1) * 100

    # Prepare output header
    echo "Folder Path,Highest % Match,Other Folder Path,Timestamp Summary,Most Common Pattern,Most Common Pattern Count,Folder1 Total Size,Folder2 Total Size" > "${OUTPUT_CSV}"

    for FOLDER1 in "${!FILE_ARRAYS[@]}"; do
      IFS=',' read -ra FILES1 <<< "${FILES_IN_FOLDER[$FOLDER1]}"
      NUM_FILES1=${#FILES1[@]}

      POPULAR_PATTERN=$(printf "%s\n" "${FILES1[@]}" | cut -d'|' -f1 | awk '{gsub(/[0-9]/, "#"); count[$0]++} END {for (word in count) print count[word], word}' | sort -n -r | head -n1)
      POPULAR_PATTERN_COUNT="$(awk '{print $1}' <<< "${POPULAR_PATTERN}")"
      POPULAR_PATTERN_REMAINDER="${POPULAR_PATTERN//^[ ]*[0-9]* }"
      [[ "${POPULAR_PATTERN_COUNT}" == "1" ]] && POPULAR_PATTERN_REMAINDER=""

      BEST_MATCH_PCT=0
      BEST_MATCH_FOLDER=""
      BEST_TIMESTAMP_SUMMARY=""
      BEST_MATCH_FILES=0

      for FOLDER2 in "${!FILE_ARRAYS[@]}"; do
        [[ "$FOLDER1" == "$FOLDER2" ]] && continue

        IFS=',' read -ra FILES2 <<< "${FILES_IN_FOLDER[$FOLDER2]}"
        NUM_FILES2=${#FILES2[@]}

        # Build associative array of FOLDER2 files for quick lookup
        declare -A FILES2_MAP
        for FILEINFO in "${FILES2[@]}"; do
          FILES2_MAP["$FILEINFO"]=1
        done

        # Count duplicates
        DUPLICATE_COUNT=0
        declare -a FOLDER1_MOD_TIMES=()
        declare -a FOLDER1_ACC_TIMES=()
        declare -a FOLDER2_MOD_TIMES=()
        declare -a FOLDER2_ACC_TIMES=()

        for FILEINFO in "${FILES1[@]}"; do
          if [[ -n "${FILES2_MAP[$FILEINFO]}" ]]; then
            ((DUPLICATE_COUNT++))
            # Extract name and size from FILEINFO (name|size)
            NAME="${FILEINFO%%|*}"
            SIZE="${FILEINFO##*|}"

            # Get timestamps
            IFS='|' read -r MOD1 ACC1 <<< "${FILE_DETAILS[$FOLDER1|$NAME|$SIZE]}"
            IFS='|' read -r MOD2 ACC2 <<< "${FILE_DETAILS[$FOLDER2|$NAME|$SIZE]}"

            FOLDER1_MOD_TIMES+=("$MOD1")
            FOLDER1_ACC_TIMES+=("$ACC1")
            FOLDER2_MOD_TIMES+=("$MOD2")
            FOLDER2_ACC_TIMES+=("$ACC2")
          fi
        done

        if (( DUPLICATE_COUNT > 0 )); then
          PCT=$(( 100 * DUPLICATE_COUNT / NUM_FILES1 ))

          if (( PCT > BEST_MATCH_PCT )); then
            BEST_MATCH_PCT=$PCT
            BEST_MATCH_FOLDER=$FOLDER2

            # Timestamp summary: get min and max timestamps for mod and access in each folder for duplicates
            MIN_MOD1=$(printf '%s\n' "${FOLDER1_MOD_TIMES[@]}" | sort | head -n1)
            MAX_MOD1=$(printf '%s\n' "${FOLDER1_MOD_TIMES[@]}" | sort | tail -n1)
            MIN_ACC1=$(printf '%s\n' "${FOLDER1_ACC_TIMES[@]}" | sort | head -n1)
            MAX_ACC1=$(printf '%s\n' "${FOLDER1_ACC_TIMES[@]}" | sort | tail -n1)

            MIN_MOD2=$(printf '%s\n' "${FOLDER2_MOD_TIMES[@]}" | sort | head -n1)
            MAX_MOD2=$(printf '%s\n' "${FOLDER2_MOD_TIMES[@]}" | sort | tail -n1)
            MIN_ACC2=$(printf '%s\n' "${FOLDER2_ACC_TIMES[@]}" | sort | head -n1)
            MAX_ACC2=$(printf '%s\n' "${FOLDER2_ACC_TIMES[@]}" | sort | tail -n1)

            BEST_TIMESTAMP_SUMMARY="Folder1 includes timestamps from $MIN_MOD1 to $MAX_MOD1 (mod) and $MIN_ACC1 to $MAX_ACC1 (access) while Folder2 includes timestamps from $MIN_MOD2 to $MAX_MOD2 (mod) and $MIN_ACC2 to $MAX_ACC2 (access)"
          fi
        fi

        unset FILES2_MAP
      done

      if (( BEST_MATCH_PCT > 0 )); then
        SIZE1=${FOLDER_FILESIZE[$FOLDER1]}
        SIZE2=${FOLDER_FILESIZE[$BEST_MATCH_FOLDER]}
        echo "$FOLDER1,$BEST_MATCH_PCT%,$BEST_MATCH_FOLDER,$BEST_TIMESTAMP_SUMMARY,$POPULAR_PATTERN_REMAINDER,$POPULAR_PATTERN_COUNT,$SIZE1,$SIZE2" >> "${OUTPUT_CSV}"
      fi
    done
}

_make_card_inventory(){
    EXTENSION="${1}"
    FILE_INVENTORY="${2}"
    CARD_INVENTORY="${3}"
    LAST_DIR=""
    COUNTER=0
    FIRST=1
    FILE_SIZE_PROCESSING_OK="Y"
    
    TEST_FILE_SIZE="$(xsv 'select' "${FILE_SIZE_INDEX}" < <(sed -n '3p' "${FILE_INVENTORY}"))"
    if [[ "${TEST_FILE_SIZE//.}" != "${TEST_FILE_SIZE}" ]] ; then
        FILE_SIZE_PROCESSING_OK="N"
    fi

    echo "Working on finding potential directories of sequential media files."
    echo "action,path,last_modified,last_accessed,owner,first_filename,last_filename,most_common_pattern,most_common_pattern_count,total_size,mod_time_first,mod_time_last,mod_time_duration_hours,count_of_extension_match,is_it_a_card" > "${CARD_INVENTORY}"
    while read LINE ; do
        # lookup column number rather than guess that it is static.
        FILE_PATH="$(xsv 'select' "${FILE_PATH_INDEX}" <<< "${LINE//\"/\"\"}")"
        if [[ "${FILE_SIZE_PROCESSING_OK}" = "Y" ]] ; then
            FILE_SIZE="$(xsv 'select' "${FILE_SIZE_INDEX}" <<< "${LINE}")"
        fi
        LAST_MOD="$(xsv 'select' "${LAST_MOD_INDEX}" <<< "${LINE}")"
        LAST_MOD_TIMES+=("${LAST_MOD}")
        FILENAME="$(xsv 'select' "${FILE_NAME_INDEX}" <<< "${LINE}")"
        if [[ -z "${FIRST_FILENAME}" ]] ; then
            FIRST_FILENAME="${FILENAME}"
        fi
        FILENAME_LIST+=("${FILENAME}")
        if [[ "${FILE_SIZE_PROCESSING_OK}" = "Y" ]] ; then
            FILE_SIZE_TALLY=$((FILE_SIZE_TALLY+FILE_SIZE))
        fi
        ((COUNTER++))
        if [[ -z "${START_LINE}" ]] ; then
            IFS=, read -r LAST_ACCESS OWNER < <(xsv 'select' "${LAST_ACCESS_INDEX},${OWNER_INDEX}" <<< "${LINE}")
            START_LINE=",\"${FILE_PATH}\",${LAST_MOD},${LAST_ACCESS},${OWNER},\"${FILENAME}\","
            REPORTED_FIRST="${FILENAME}"
            FIRST=0
        elif [[ "${FILE_PATH}" != "${LAST_DIR}" ]] ; then
            if [[ "${COUNTER}" -gt 1 ]]; then
                POPULAR_PATTERN=$(printf "%s\n" "${FILENAME_LIST[@]}" | awk '{gsub(/[0-9]/, "#"); count[$0]++} END {for (word in count) print count[word], word}' | sort -n -r | head -n1)
                POPULAR_PATTERN_COUNT="$(awk '{print $1}' <<< "${POPULAR_PATTERN}")"
                POPULAR_PATTERN_REMAINDER="$(echo "${POPULAR_PATTERN//^[ ]*}" | cut -d' ' -f2-)"
                [[ "${POPULAR_PATTERN_COUNT}" == "1" ]] && POPULAR_PATTERN_REMAINDER=""
                declare -a matching_times=()
                for i in "${!FILENAME_LIST[@]}"; do
                    file="${FILENAME_LIST[i]}"
                    mod_time="${LAST_MOD_TIMES[i]}"
                    # Convert file to pattern format
                    file_pattern=$(echo "$file" | sed 's/[0-9]/#/g')
                    if [[ "$file_pattern" == "${POPULAR_PATTERN_REMAINDER}" ]]; then
                        matching_times+=("$mod_time")
                    fi
                done
                IFS=$'\n' sorted_times=($(sort <<<"${matching_times[*]}"))
                unset IFS
                START_MOD_DATE="${sorted_times[0]}"
                END_MOD_DATE="${sorted_times[@]: -1}"
                if [[ -n "${START_MOD_DATE}" && -n "${END_MOD_DATE}" ]] ; then
                    # Convert timestamps to epoch (seconds since 1970)
                    if [[ "$(uname)" == "Darwin" ]]; then
                        # macOS (BSD date)
                        START_EPOCH=$(date -j -f "%Y-%m-%d %H:%M:%S" "$START_MOD_DATE" "+%s")
                        END_EPOCH=$(date -j -f "%Y-%m-%d %H:%M:%S" "$END_MOD_DATE" "+%s")
                    else
                        # Linux (GNU date)
                        START_EPOCH=$(date -d "$START_MOD_DATE" "+%s")
                        END_EPOCH=$(date -d "$END_MOD_DATE" "+%s")
                    fi

                    # Check for conversion errors
                    if [[ -z "$START_EPOCH" || -z "$END_EPOCH" ]]; then
                        echo "Error: Failed to convert date to epoch time." >&2
                        exit 1
                    fi

                    # Calculate duration in seconds
                    DURATION_SECONDS=$((END_EPOCH - START_EPOCH))

                    # Convert seconds to hours (floating-point)
                    TIME_RANGE_HOURS=$((DURATION_SECONDS / 3600))
                else
                    unset DURATION_SECONDS
                    unset TIME_RANGE_HOURS
                fi
                echo -n "${START_LINE}\"${LAST_FILENAME}\",\"${POPULAR_PATTERN_REMAINDER}\",${POPULAR_PATTERN_COUNT},${FILE_SIZE_TALLY},${START_MOD_DATE},${END_MOD_DATE},${TIME_RANGE_HOURS},${COUNTER},"

                # files are renamed all the time, instead of first vs last try to match on most used pattern.
                
                # score the camera card likeliness
                LIKELY=0
                if [[ -n "${POPULAR_PATTERN_REMAINDER}" ]] ; then
                    ((LIKELY++))
                fi
                if [[ $TIME_RANGE_HOURS -lt 48 ]]; then
                    ((LIKELY++))
                fi
                # add report upon the proofsheet
                #if [[ "${REPORTED_FIRST//[0-9 ]}" == "${LAST_FILENAME//[0-9 ]}" ]] ; then
                    #((LIKELY++))
                    #fi
                if [[ "${COUNTER}" != "1" ]] ; then
                    ((LIKELY++))
                fi
                echo "${LIKELY}" >> "${CARD_INVENTORY}"
            fi
            COUNTER="0"
            unset FIRST_FILENAME
            unset LAST_MOD_TIMES
            # why did this line come out 	//audio.cz.rferl.org/videosrv/Video-1/Central Newsroom/2018/06_2018/Halabja Doc Backup/KURD BACKUP VIDEO ROMAN	2/21/18	6/4/18	digainst	[569 Files]	RFERL4960.MXF	RFERL####.MXF	569		570	probably not
            # 	//audio.cz.rferl.org/videosrv/Video-1/Services' Projects/KAZ/2018/10_2018/Aral film	10/5/18	10/8/18	KupkaR	[372 Files]	RFERL6723.MXF	RFERL####.MXF	372		373	probably not

            # Oleksander comments
            # treesize can export date and time. so can group videos by being in a pattern.
            # only parse csv line if the data will be used
            # can we remove if only 1 file per folder and if no pattern.
            # add most common time range like the most common pattern, 
            # get Folder Path in treesize, 

            # card criteria: 1. filename pattern, last mod date time, and file count.
            IFS=, read -r LAST_ACCESS OWNER < <(xsv 'select' "${LAST_ACCESS_INDEX},${OWNER_INDEX}" <<< "${LINE}")
            START_LINE=",\"${FILE_PATH}\",${LAST_MOD},${LAST_ACCESS},${OWNER},\"${FILENAME}\","
            REPORTED_FIRST="${FILENAME}"
            unset FILENAME_LIST FILE_SIZE_TALLY 
        fi
        # add tally of the filesize
        # add column if already processed

        ## refactor csv to xml
        LAST_DIR="${FILE_PATH}"
        LAST_FILENAME="${FILENAME}"
    done < <(sed 's|\\|/|g' "${FILE_INVENTORY}" | tail -n +2) >> "${CARD_INVENTORY}"
    if [[ "${COUNTER}" -gt 1 ]]; then
        POPULAR_PATTERN=$(printf "%s\n" "${FILENAME_LIST[@]}" | awk '{gsub(/[0-9]/, "#"); count[$0]++} END {for (word in count) print count[word], word}' | sort -n -r | head -n1)
        POPULAR_PATTERN_COUNT="$(awk '{print $1}' <<< "${POPULAR_PATTERN}")"
        POPULAR_PATTERN_REMAINDER="${POPULAR_PATTERN//^[ ]*[0-9]* }"
        [[ "${POPULAR_PATTERN_COUNT}" == "1" ]] && POPULAR_PATTERN_REMAINDER=""
        echo -n "${START_LINE}\"${LAST_FILENAME}\",\"${POPULAR_PATTERN_REMAINDER}\",${POPULAR_PATTERN_COUNT},${FILE_SIZE_TALLY},${START_MOD_DATE_FORMATTED},${END_MOD_DATE_FORMATTED},${TIME_RANGE_HOURS},${COUNTER}," >> "${CARD_INVENTORY}"

        # score the camera card likeliness
        LIKELY=0
        if [[ -n "${POPULAR_PATTERN_REMAINDER}" ]] ; then
            ((LIKELY++))
        fi
        if [[ $TIME_RANGE_HOURS -lt 48 ]]; then
            ((LIKELY++))
        fi
        # add report upon the proofsheet
        if [[ "${REPORTED_FIRST//[0-9 ]}" == "${LAST_FILENAME//[0-9 ]}" ]] ; then
            ((LIKELY++))
        fi
        if [[ "${COUNTER}" != "1" ]] ; then
            ((LIKELY++))
        fi
        echo "${LIKELY}" >> "${CARD_INVENTORY}"
    fi
    echo "Done, check out ${CARD_INVENTORY}."
    # for log coming out of cameracards output, add the input list with filename,in,out,duration.
}

_report_on_service_files(){
    # add filter to give an age date.
    # one variable for mod date and one for access date
    CSV_HEADER="$(head -n 1 "${MEDIA_INVENTORY}")"
    LIKELY_SERVICE_FILES="${TREESIZE_CSV%.*}.LIKELY_SERVICE_FILES.csv"
    echo "Classification,${CSV_HEADER}" > "${LIKELY_SERVICE_FILES}"
    grep --no-filename "^AFP[ -]TV" "${MEDIA_INVENTORY}" | sed 's|^|Likely AFP,|g' >> "${LIKELY_SERVICE_FILES}"
    grep --no-filename "^[0-9][0-9][0-9][0-9][0-9][0-9][0-9]_[0-9a-z]" "${MEDIA_INVENTORY}" | sed 's|^|Likely AP,|g' >> "${LIKELY_SERVICE_FILES}"
    grep --no-filename "[0-9]_[0-9][0-9][0-9][0-9][0-9][0-9][0-9]_[0-9a-z]" "${MEDIA_INVENTORY}" | sed 's|^|Likely AP,|g' >> "${LIKELY_SERVICE_FILES}"
    grep --no-filename "^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9][0-9][0-9][0-9][0-9]Z_" "${MEDIA_INVENTORY}" | sed 's|^|Likely Reuters,|g' >> "${LIKELY_SERVICE_FILES}"
    grep --no-filename "^Storyful" "${MEDIA_INVENTORY}" | sed 's|^|Likely Storyful,|g' >> "${LIKELY_SERVICE_FILES}"
    echo "Found $(wc -l "${LIKELY_SERVICE_FILES}" | awk '{print $1}') potential service files."
    echo "$(grep -i "^Likely AFP,"     "${LIKELY_SERVICE_FILES}" | wc -l) AFP files"
    echo "$(grep -i "^Likely AP,"      "${LIKELY_SERVICE_FILES}" | wc -l) AP files"
    echo "$(grep -i "^Likely Reuters," "${LIKELY_SERVICE_FILES}" | wc -l) Reuters files"
    echo "$(grep -i "^Likely Storyful" "${LIKELY_SERVICE_FILES}" | wc -l) Storyful files"
}

_split_csv(){
    TREESIZE_CSV4SPLIT="${1}"
    MEDIA_INVENTORY4SPLIT="${2}"
    if [[ ! -f "${MEDIA_INVENTORY4SPLIT}" ]] ; then
        echo "Splitting out an inventory of media files"
        echo "${HEADER_LINE}" > "${MEDIA_INVENTORY4SPLIT}"
        grep -i ".\(MOV\|MP4\|MTS\|MXF\)," "${TREESIZE_CSV4SPLIT}" | xsv sort --select "${FILE_PATH_INDEX},${FILE_NAME_INDEX}" >> "${MEDIA_INVENTORY4SPLIT}"
        echo "Found $(wc -l "${MEDIA_INVENTORY4SPLIT}" | awk '{print $1}') media files."
        echo "   $(xsv \select "${FILE_NAME_INDEX}" "${MEDIA_INVENTORY4SPLIT}" | grep -i ".MOV$" -c) mov files"
        echo "   $(xsv \select "${FILE_NAME_INDEX}" "${MEDIA_INVENTORY4SPLIT}" | grep -i ".MP4$" -c) mp4 files"
        echo "   $(xsv \select "${FILE_NAME_INDEX}" "${MEDIA_INVENTORY4SPLIT}" | grep -i ".MTS$" -c) mts files"
        echo "   $(xsv \select "${FILE_NAME_INDEX}" "${MEDIA_INVENTORY4SPLIT}" | grep -i ".MXF$" -c) mxf files"
    fi
}

_usage(){
cat <<EOF
$(basename "${0}")

This script accepts a treesize report as an input and generates a list of resummarized outputs. Including:
- DUPLICATES.csv            - A summary of folders that contain similar contents.
- LIKELY_SERVICE_FILES.csv  - A list of files that match the naming patterns of news agency services.
- MEDIA_FILES.csv           - A list of files from the treesize report that are mov, m2t, mxf, or mkv files.
- MEDIA_CARDS.csv           - A list of directories that contain video files in a pattern which may be camera card directories.

Dependencies: ${DEPENDENCIES[@]}

Usage: $(basename "${0}") -m MEDIAID [ -o AIP_DESTINATION ] [-tca] /path/to/input/camera_card_directory [ /path/to/input/camera_card_directory_2... ]
  -h display this help
  -x only generate DUPLICATES.csv and LIKELY_SERVICE_FILES.csv and then stop (skipping the other outputs).
EOF
    exit
}

# getopts loop
OPTIND=1
while getopts ":xh" OPT; do
    case "${OPT}" in
        x) JUST_SOME="Y" ;;
        h) _usage ;;  # if the operator runs "[scriptname] -h" then the _usage text above will display in the terminal
        *) echo "Invalid option -${OPTARG}" ; _usage ;; # if the operator tries to use an option other than the ones listed above, the _usage text will display in the terminal
    esac
done
shift $(( OPTIND - 1 ))
INPUT_COUNT="$#"
while [[ "${@}" != "" ]] ; do
INPUT="${1}"
INPUT_EXT="${INPUT##*.}"
shift
echo "--------------------------------------------------------------------------"
if [[ -d "${INPUT}" ]] ; then
    echo "Recognizing the input ($(basename "${INPUT}")) as a directory, but this isn't set up yet. Exiting."
    exit
elif [[ "${INPUT_EXT}" = "xlsx" ]] ; then
    echo "Recognizing the input ($(basename "${INPUT}")) as an excel file. Perhaps a tree size report."
    TREESIZE_CSV="${INPUT%.*}.csv"
    if [[ -f "${TREESIZE_CSV}" ]] ; then
        echo "Seems like a csv was already made from the treesize report at ${TREESIZE_CSV}."
    else
        echo "Making a csv from the treesize report..."
        INPUT_SHEETS="$(in2csv -n "${INPUT}")"
        SHEET_NAME="$(echo "${INPUT_SHEETS}" | grep "^20" | head -n 1)"
        if [[ -n "${SHEET_NAME}" ]] ; then
            echo "This seems to be a treesize report with an inventory on the sheet named ${SHEET_NAME}."
        else
            echo "Hmmm, no file inventory found. Perhaps this isn't a treesize report."
            exit
        fi
        in2csv --no-header-row --sheet "${SHEET_NAME}" "${INPUT}" > "${TREESIZE_CSV}"
    fi
elif [[ "${INPUT_EXT}" = "csv" ]] ; then
    echo "Recognizing the input ($(basename "${INPUT}")) as a csv."
    TREESIZE_CSV="${INPUT}"
else
    echo "Hmmm, not sure what ($(basename "${INPUT}")) is. Exiting."
    exit
fi
HEADER_LINE="$(sed '2q;d' "${TREESIZE_CSV}")"
FILE_NAME_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Name' | cut -d: -f1)"
FILE_PATH_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Folder Path' | cut -d: -f1)"
if [[ -z "${FILE_PATH_INDEX}" ]] ; then
    FILE_PATH_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Full Path' | cut -d: -f1)"
fi
FILE_SIZE_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Size' | cut -d: -f1)"
LAST_MOD_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Last Modified' | cut -d: -f1)"
LAST_ACCESS_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Last Accessed' | cut -d: -f1)"
OWNER_INDEX="$(echo "${HEADER_LINE}" | sed -n $'1s/,/\\\n/gp' | grep -nx 'Owner' | cut -d: -f1)"

MEDIA_INVENTORY="${TREESIZE_CSV%.*}.MEDIA_FILES.csv"
_split_csv "${TREESIZE_CSV}" "${MEDIA_INVENTORY}"
MEDIA_INVENTORY_CSV_LIST+=("${MEDIA_INVENTORY}")
MEDIA_INVENTORY_CSV_LISTNAME="${MEDIA_INVENTORY_CSV_LISTNAME}_$(basename "${INPUT%.*}")"
_report_on_service_files
_report_duplicates "${MEDIA_INVENTORY}" "${TREESIZE_CSV%.*}.DUPLICATES.csv"
if [[ "${JUST_SOME}" = "Y" ]] ; then
    echo "since -x was used will skip the rest of the reports"
    continue
fi

MEDIA_CARD_INVENTORY="${TREESIZE_CSV%.*}.MEDIA_CARDS.csv"
_make_card_inventory "MOV" "${MEDIA_INVENTORY}" "${MEDIA_CARD_INVENTORY}"

done
if [[ "${INPUT_COUNT}" -gt 1 ]] ; then
    echo "--------------------------------------------------------------------------"
    echo "Now finding duplicates across the input treesize reports and writing to ${MEDIA_INVENTORY_CSV_LISTNAME}_SPANNING_DUPLICATES.csv"
    echo "Inputs to duplicate test: ${MEDIA_INVENTORY_CSV_LIST[@]}"
    cat "${MEDIA_INVENTORY_CSV_LIST[@]}" > "${MEDIA_INVENTORY_CSV_LISTNAME}_SPANNING.csv"
    _report_duplicates "${MEDIA_INVENTORY_CSV_LISTNAME}_SPANNING.csv" "${MEDIA_INVENTORY_CSV_LISTNAME}_SPANNING_DUPLICATES.csv"
fi
